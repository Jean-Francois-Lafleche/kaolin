

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>models.PointNet &mdash; kaolin 0.1.0 alpha documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../_static/language_data.js"></script>
        <script type="text/javascript" src="../../_static/clipboard.min.js"></script>
        <script type="text/javascript" src="../../_static/copybutton.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/copybutton.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home" alt="Documentation Home"> kaolin
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../notes/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/introduction.html">Kaolin: An introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/hello3d.html">Hello, 3D world!</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/usd_tutorial.html">USD Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/datasets_tutorial.html">Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/conversions_tutorial.html">Conversions across various representations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/vision_tutorial.html">Computer vision functionality</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/graphics_tutorial.html">Graphics functionality</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/differentiable_rendering.html">Differentiable rendering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/pointnet.html">PointNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/pixel2mesh.html">Pixel2Mesh</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/geometrics.html">GEOMetrics</a></li>
</ul>
<p class="caption"><span class="caption-text">API Reference:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../modules/conversions.html">kaolin.conversions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../modules/datasets.html">kaolin.datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../modules/graphics.html">kaolin.graphics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../modules/mathutils.html">kaolin.mathutils</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../modules/metrics.html">kaolin.metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../modules/models.html">kaolin.models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../modules/rep.html">kaolin.rep</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../modules/transforms.html">kaolin.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../modules/vision.html">kaolin.vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../modules/visualize.html">kaolin.visualize</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">kaolin</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../index.html">Module code</a> &raquo;</li>
        
      <li>models.PointNet</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for models.PointNet</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>

<span class="kn">from</span> <span class="nn">typing</span> <span class="k">import</span> <span class="n">Iterable</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>


<div class="viewcode-block" id="PointNetFeatureExtractor"><a class="viewcode-back" href="../../modules/models.html#models.PointNet.PointNetFeatureExtractor">[docs]</a><span class="k">class</span> <span class="nc">PointNetFeatureExtractor</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;PointNet feature extractor (extracts either global or local, i.e.,</span>
<span class="sd">    per-point features).</span>

<span class="sd">    Based on the original PointNet paper:.</span>

<span class="sd">    .. note::</span>

<span class="sd">        If you use this code, please cite the original paper in addition to Kaolin.</span>
<span class="sd">        </span>
<span class="sd">        .. code-block::</span>

<span class="sd">            @article{qi2016pointnet,</span>
<span class="sd">              title={PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation},</span>
<span class="sd">              author={Qi, Charles R and Su, Hao and Mo, Kaichun and Guibas, Leonidas J},</span>
<span class="sd">              journal={arXiv preprint arXiv:1612.00593},</span>
<span class="sd">              year={2016}</span>
<span class="sd">            }</span>

<span class="sd">    Args:</span>
<span class="sd">        in_channels (int): Number of channels in the input pointcloud</span>
<span class="sd">            (default: 3, for X, Y, Z coordinates respectively).</span>
<span class="sd">        feat_size (int): Size of the global feature vector</span>
<span class="sd">            (default: 1024)</span>
<span class="sd">        layer_dims (Iterable[int]): Sizes of fully connected layers</span>
<span class="sd">            to be used in the feature extractor (excluding the input and</span>
<span class="sd">            the output layer sizes). Note: the number of</span>
<span class="sd">            layers in the feature extractor is implicitly parsed from</span>
<span class="sd">            this variable.</span>
<span class="sd">        global_feat (bool): Extract global features (i.e., one feature</span>
<span class="sd">            for the entire pointcloud) if set to True. If set to False,</span>
<span class="sd">            extract per-point (local) features (default: True).</span>
<span class="sd">        activation (function): Nonlinearity to be used as activation</span>
<span class="sd">                    function after each batchnorm (default: F.relu)</span>
<span class="sd">        batchnorm (bool): Whether or not to use batchnorm layers</span>
<span class="sd">            (default: True)</span>
<span class="sd">        transposed_input (bool): Whether the input&#39;s second and third dimension</span>
<span class="sd">            is already transposed. If so, a transpose operation can be avoided,</span>
<span class="sd">            improving performance.</span>
<span class="sd">            See documentation for the forward method for more details.</span>

<span class="sd">    For example, to specify a PointNet feature extractor with 4 linear</span>
<span class="sd">    layers (sizes 6 -&gt; 10, 10 -&gt; 40, 40 -&gt; 500, 500 -&gt; 1024), with</span>
<span class="sd">    3 input channels in the pointcloud and a global feature vector of size</span>
<span class="sd">    1024, see the example below.</span>

<span class="sd">    Example:</span>

<span class="sd">        &gt;&gt;&gt; pointnet = PointNetFeatureExtractor(in_channels=3, feat_size=1024,</span>
<span class="sd">                                           layer_dims=[10, 20, 40, 500])</span>
<span class="sd">        &gt;&gt;&gt; x = torch.rand(2, 3, 30)</span>
<span class="sd">        &gt;&gt;&gt; y = pointnet(x)</span>
<span class="sd">        print(y.shape)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
                 <span class="n">feat_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span>
                 <span class="n">layer_dims</span><span class="p">:</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">],</span>
                 <span class="n">global_feat</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                 <span class="n">activation</span><span class="o">=</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span>
                 <span class="n">batchnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                 <span class="n">transposed_input</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PointNetFeatureExtractor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Argument in_channels expected to be of type int. &#39;</span>
                            <span class="s1">&#39;Got </span><span class="si">{0}</span><span class="s1"> instead.&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">in_channels</span><span class="p">)))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">feat_size</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Argument feat_size expected to be of type int. &#39;</span>
                            <span class="s1">&#39;Got </span><span class="si">{0}</span><span class="s1"> instead.&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">feat_size</span><span class="p">)))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">layer_dims</span><span class="p">,</span> <span class="s1">&#39;__iter__&#39;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Argument layer_dims is not iterable.&#39;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">layer_dim</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">layer_dims</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer_dim</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Elements of layer_dims must be of type int. &#39;</span>
                                <span class="s1">&#39;Found type </span><span class="si">{0}</span><span class="s1"> at index </span><span class="si">{1}</span><span class="s1">.&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                                    <span class="nb">type</span><span class="p">(</span><span class="n">layer_dim</span><span class="p">),</span> <span class="n">idx</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">global_feat</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Argument global_feat expected to be of type &#39;</span>
                            <span class="s1">&#39;bool. Got </span><span class="si">{0}</span><span class="s1"> instead.&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                                <span class="nb">type</span><span class="p">(</span><span class="n">global_feat</span><span class="p">)))</span>

        <span class="c1"># Store feat_size as a class attribute</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">feat_size</span> <span class="o">=</span> <span class="n">feat_size</span>

        <span class="c1"># Store activation as a class attribute</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">activation</span>

        <span class="c1"># Store global_feat as a class attribute</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">global_feat</span> <span class="o">=</span> <span class="n">global_feat</span>

        <span class="c1"># Add in_channels to the head of layer_dims (the first layer</span>
        <span class="c1"># has number of channels equal to `in_channels`). Also, add</span>
        <span class="c1"># feat_size to the tail of layer_dims.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer_dims</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="n">layer_dims</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">layer_dims</span><span class="p">)</span>
        <span class="n">layer_dims</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">)</span>
        <span class="n">layer_dims</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">feat_size</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">conv_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">batchnorm</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bn_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">layer_dims</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">conv_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="n">layer_dims</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span>
                                              <span class="n">layer_dims</span><span class="p">[</span><span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">batchnorm</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">bn_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">layer_dims</span><span class="p">[</span><span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]))</span>

        <span class="c1"># Store whether or not to use batchnorm as a class attribute</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batchnorm</span> <span class="o">=</span> <span class="n">batchnorm</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">transposed_input</span> <span class="o">=</span> <span class="n">transposed_input</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Forward pass through the PointNet feature extractor.</span>

<span class="sd">        Args:</span>
<span class="sd">            x (torch.Tensor): Tensor representing a pointcloud</span>
<span class="sd">                (shape: :math:`B \times N \times D`, where :math:`B`</span>
<span class="sd">                is the batchsize, :math:`N` is the number of points</span>
<span class="sd">                in the pointcloud, and :math:`D` is the dimensionality</span>
<span class="sd">                of each point in the pointcloud).</span>
<span class="sd">                If self.transposed_input is True, then the shape is</span>
<span class="sd">                :math:`B \times D \times N`.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">transposed_input</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Number of points</span>
        <span class="n">num_points</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>

        <span class="c1"># By default, initialize local features (per-point features)</span>
        <span class="c1"># to None.</span>
        <span class="n">local_features</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Apply a sequence of conv-batchnorm-nonlinearity operations</span>

        <span class="c1"># For the first layer, store the features, as these will be</span>
        <span class="c1"># used to compute local features (if specified).</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batchnorm</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bn_layers</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="bp">self</span><span class="o">.</span><span class="n">conv_layers</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">x</span><span class="p">)))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv_layers</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">x</span><span class="p">))</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">global_feat</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
            <span class="n">local_features</span> <span class="o">=</span> <span class="n">x</span>

        <span class="c1"># Pass through the remaining layers (until the penultimate layer).</span>
        <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv_layers</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batchnorm</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bn_layers</span><span class="p">[</span><span class="n">idx</span><span class="p">](</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">conv_layers</span><span class="p">[</span><span class="n">idx</span><span class="p">](</span><span class="n">x</span><span class="p">)))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv_layers</span><span class="p">[</span><span class="n">idx</span><span class="p">](</span><span class="n">x</span><span class="p">))</span>

        <span class="c1"># For the last layer, do not apply nonlinearity.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batchnorm</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">](</span><span class="bp">self</span><span class="o">.</span><span class="n">conv_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># Max pooling.</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">feat_size</span><span class="p">)</span>

        <span class="c1"># If extracting global features, return at this point.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">global_feat</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">x</span>

        <span class="c1"># If extracting local features, compute local features by</span>
        <span class="c1"># concatenating global features, and per-point features</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">feat_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_points</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">local_features</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></div>


<div class="viewcode-block" id="PointNetClassifier"><a class="viewcode-back" href="../../modules/models.html#models.PointNet.PointNetClassifier">[docs]</a><span class="k">class</span> <span class="nc">PointNetClassifier</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;PointNet classifier. Uses the PointNet feature extractor, and</span>
<span class="sd">    adds classification layers on top.</span>

<span class="sd">    .. note::</span>

<span class="sd">        If you use this code, please cite the original paper in addition to Kaolin.</span>
<span class="sd">        </span>
<span class="sd">        .. code-block::</span>

<span class="sd">            @article{qi2016pointnet,</span>
<span class="sd">              title={PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation},</span>
<span class="sd">              author={Qi, Charles R and Su, Hao and Mo, Kaichun and Guibas, Leonidas J},</span>
<span class="sd">              journal={arXiv preprint arXiv:1612.00593},</span>
<span class="sd">              year={2016}</span>
<span class="sd">            }</span>

<span class="sd">    Args:</span>
<span class="sd">        in_channels (int): Number of channels in the input pointcloud</span>
<span class="sd">            (default: 3, for X, Y, Z coordinates respectively).</span>
<span class="sd">        feat_size (int): Size of the global feature vector</span>
<span class="sd">            (default: 1024)</span>
<span class="sd">        num_classes (int): Number of classes (for the classification</span>
<span class="sd">            task) (default: 2).</span>
<span class="sd">        dropout (float): Dropout ratio to use (default: 0.). Note: If</span>
<span class="sd">            the ratio is set to 0., we altogether skip using a dropout</span>
<span class="sd">            layer.</span>
<span class="sd">        layer_dims (Iterable[int]): Sizes of fully connected layers</span>
<span class="sd">            to be used in the feature extractor (excluding the input and</span>
<span class="sd">            the output layer sizes). Note: the number of</span>
<span class="sd">            layers in the feature extractor is implicitly parsed from</span>
<span class="sd">            this variable.</span>
<span class="sd">        activation (function): Nonlinearity to be used as activation</span>
<span class="sd">                    function after each batchnorm (default: F.relu)</span>
<span class="sd">        batchnorm (bool): Whether or not to use batchnorm layers</span>
<span class="sd">            (default: True)</span>
<span class="sd">        transposed_input (bool): Whether the input&#39;s second and third dimension</span>
<span class="sd">            is already transposed. If so, a transpose operation can be avoided,</span>
<span class="sd">            improving performance.</span>
<span class="sd">            See documentation of PointNetFeatureExtractor for more details.</span>

<span class="sd">    Example:</span>

<span class="sd">        pointnet = PointNetClassifier(in_channels=6, feat_size=1024,</span>
<span class="sd">                                      feat_layer_dims=[32, 64, 256],</span>
<span class="sd">                                      classifier_layer_dims=[500, 200, 100])</span>
<span class="sd">        x = torch.rand(5, 6, 30)</span>
<span class="sd">        y = pointnet(x)</span>
<span class="sd">        print(y.shape)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
                 <span class="n">feat_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span>
                 <span class="n">num_classes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
                 <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.</span><span class="p">,</span>
                 <span class="n">classifier_layer_dims</span><span class="p">:</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">512</span><span class="p">,</span> <span class="mi">256</span><span class="p">],</span>
                 <span class="n">feat_layer_dims</span><span class="p">:</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">],</span>
                 <span class="n">activation</span><span class="o">=</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span>
                 <span class="n">batchnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                 <span class="n">transposed_input</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">PointNetClassifier</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Argument num_classes must be of type int. &#39;</span>
                            <span class="s1">&#39;Got </span><span class="si">{0}</span><span class="s1"> instead.&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">num_classes</span><span class="p">)))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dropout</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Argument dropout must be of type float. &#39;</span>
                            <span class="s1">&#39;Got </span><span class="si">{0}</span><span class="s1"> instead.&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">dropout</span><span class="p">)))</span>
        <span class="k">if</span> <span class="n">dropout</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">dropout</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Dropout ratio must always be in the range&#39;</span>
                             <span class="s1">&#39;[0, 1]. Got </span><span class="si">{0}</span><span class="s1"> instead.&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">dropout</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">classifier_layer_dims</span><span class="p">,</span> <span class="s1">&#39;__iter__&#39;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Argument classifier_layer_dims is not iterable.&#39;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">layer_dim</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">classifier_layer_dims</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer_dim</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Expected classifier_layer_dims to contain &#39;</span>
                                <span class="s1">&#39;int. Found type </span><span class="si">{0}</span><span class="s1"> at index </span><span class="si">{1}</span><span class="s1">.&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                                    <span class="nb">type</span><span class="p">(</span><span class="n">layer_dim</span><span class="p">),</span> <span class="n">idx</span><span class="p">))</span>

        <span class="c1"># Add feat_size to the head of classifier_layer_dims (the output of</span>
        <span class="c1"># the PointNet feature extractor has number of elements equal to</span>
        <span class="c1"># has number of channels equal to `in_channels`).</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">classifier_layer_dims</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="n">classifier_layer_dims</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">classifier_layer_dims</span><span class="p">)</span>
        <span class="n">classifier_layer_dims</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">feat_size</span><span class="p">)</span>

        <span class="c1"># Note that `global_feat` MUST be set to True, for global</span>
        <span class="c1"># classification tasks.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">feature_extractor</span> <span class="o">=</span> <span class="n">PointNetFeatureExtractor</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">feat_size</span><span class="o">=</span><span class="n">feat_size</span><span class="p">,</span>
            <span class="n">layer_dims</span><span class="o">=</span><span class="n">feat_layer_dims</span><span class="p">,</span> <span class="n">global_feat</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span> <span class="n">batchnorm</span><span class="o">=</span><span class="n">batchnorm</span><span class="p">,</span>
            <span class="n">transposed_input</span><span class="o">=</span><span class="n">transposed_input</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">linear_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">batchnorm</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bn_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">classifier_layer_dims</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">linear_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">classifier_layer_dims</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span>
                                                <span class="n">classifier_layer_dims</span><span class="p">[</span><span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]))</span>
            <span class="k">if</span> <span class="n">batchnorm</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">bn_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span>
                    <span class="n">classifier_layer_dims</span><span class="p">[</span><span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">last_linear_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">classifier_layer_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                                           <span class="n">num_classes</span><span class="p">)</span>

        <span class="c1"># Store activation as a class attribute</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">activation</span>

        <span class="c1"># Dropout layer (if dropout ratio is in the interval (0, 1]).</span>
        <span class="k">if</span> <span class="n">dropout</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Store whether or not to use batchnorm as a class attribute</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batchnorm</span> <span class="o">=</span> <span class="n">batchnorm</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">transposed_input</span> <span class="o">=</span> <span class="n">transposed_input</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Forward pass through the PointNet classifier.</span>

<span class="sd">        Args:</span>
<span class="sd">            x (torch.Tensor): Tensor representing a pointcloud</span>
<span class="sd">                (shape: :math:`B \times N \times D`, where :math:`B`</span>
<span class="sd">                is the batchsize, :math:`N` is the number of points</span>
<span class="sd">                in the pointcloud, and :math:`D` is the dimensionality</span>
<span class="sd">                of each point in the pointcloud).</span>
<span class="sd">                If self.transposed_input is True, then the shape is</span>
<span class="sd">                :math:`B \times D \times N`.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_extractor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_layers</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batchnorm</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bn_layers</span><span class="p">[</span><span class="n">idx</span><span class="p">](</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">linear_layers</span><span class="p">[</span><span class="n">idx</span><span class="p">](</span><span class="n">x</span><span class="p">)))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_layers</span><span class="p">[</span><span class="n">idx</span><span class="p">](</span><span class="n">x</span><span class="p">))</span>
        <span class="c1"># For penultimate linear layer, apply dropout before batchnorm</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batchnorm</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bn_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">](</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">linear_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">))))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">)))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batchnorm</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bn_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">](</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">linear_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">)))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">))</span>
        <span class="c1"># TODO: Use dropout before batchnorm of penultimate linear layer</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_linear_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># return F.log_softmax(x, dim=1)</span>
        <span class="k">return</span> <span class="n">x</span></div>


<div class="viewcode-block" id="PointNetSegmenter"><a class="viewcode-back" href="../../modules/models.html#models.PointNet.PointNetSegmenter">[docs]</a><span class="k">class</span> <span class="nc">PointNetSegmenter</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;PointNet segmenter. Uses the PointNet feature extractor, and</span>
<span class="sd">    adds per-point segmentation layers on top.</span>

<span class="sd">    .. note::</span>

<span class="sd">        If you use this code, please cite the original paper in addition to Kaolin.</span>
<span class="sd">        </span>
<span class="sd">        .. code-block::</span>

<span class="sd">            @article{qi2016pointnet,</span>
<span class="sd">              title={PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation},</span>
<span class="sd">              author={Qi, Charles R and Su, Hao and Mo, Kaichun and Guibas, Leonidas J},</span>
<span class="sd">              journal={arXiv preprint arXiv:1612.00593},</span>
<span class="sd">              year={2016}</span>
<span class="sd">            }</span>

<span class="sd">    Args:</span>
<span class="sd">        in_channels (int): Number of channels in the input pointcloud</span>
<span class="sd">            (default: 3, for X, Y, Z coordinates respectively).</span>
<span class="sd">        feat_size (int): Size of the global feature vector</span>
<span class="sd">            (default: 1024)</span>
<span class="sd">        num_classes (int): Number of classes (for the segmentation</span>
<span class="sd">            task) (default: 2).</span>
<span class="sd">        dropout (float): Dropout ratio to use (default: 0.). Note: If</span>
<span class="sd">            the ratio is set to 0., we altogether skip using a dropout</span>
<span class="sd">            layer.</span>
<span class="sd">        layer_dims (Iterable[int]): Sizes of fully connected layers</span>
<span class="sd">            to be used in the feature extractor (excluding the input and</span>
<span class="sd">            the output layer sizes). Note: the number of</span>
<span class="sd">            layers in the feature extractor is implicitly parsed from</span>
<span class="sd">            this variable.</span>
<span class="sd">        activation (function): Nonlinearity to be used as activation</span>
<span class="sd">                    function after each batchnorm (default: F.relu)</span>
<span class="sd">        batchnorm (bool): Whether or not to use batchnorm layers</span>
<span class="sd">            (default: True)</span>
<span class="sd">        transposed_input (bool): Whether the input&#39;s second and third dimension</span>
<span class="sd">            is already transposed. If so, a transpose operation can be avoided,</span>
<span class="sd">            improving performance.</span>
<span class="sd">            See documentation of PointNetFeatureExtractor for more details.</span>

<span class="sd">    Example:</span>

<span class="sd">        pointnet = PointNetSegmenter(in_channels=6, feat_size=1024,</span>
<span class="sd">                                         feat_layer_dims=[32, 64, 256],</span>
<span class="sd">                                         classifier_layer_dims=[500, 200, 100])</span>
<span class="sd">        x = torch.rand(5, 6, 30)</span>
<span class="sd">        y = pointnet(x)</span>
<span class="sd">        print(y.shape)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
                 <span class="n">feat_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span>
                 <span class="n">num_classes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
                 <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.</span><span class="p">,</span>
                 <span class="n">classifier_layer_dims</span><span class="p">:</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">512</span><span class="p">,</span> <span class="mi">256</span><span class="p">],</span>
                 <span class="n">feat_layer_dims</span><span class="p">:</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">],</span>
                 <span class="n">activation</span><span class="o">=</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span>
                 <span class="n">batchnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                 <span class="n">transposed_input</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PointNetSegmenter</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Argument num_classes must be of type int. &#39;</span>
                            <span class="s1">&#39;Got </span><span class="si">{0}</span><span class="s1"> instead.&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">num_classes</span><span class="p">)))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dropout</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Argument dropout must be of type float. &#39;</span>
                            <span class="s1">&#39;Got </span><span class="si">{0}</span><span class="s1"> instead.&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">dropout</span><span class="p">)))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">classifier_layer_dims</span><span class="p">,</span> <span class="s1">&#39;__iter__&#39;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Argument classifier_layer_dims is not iterable.&#39;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">layer_dim</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">classifier_layer_dims</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer_dim</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Expected classifier_layer_dims to contain &#39;</span>
                                <span class="s1">&#39;int. Found type </span><span class="si">{0}</span><span class="s1"> at index </span><span class="si">{1}</span><span class="s1">.&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                                    <span class="nb">type</span><span class="p">(</span><span class="n">layer_dim</span><span class="p">),</span> <span class="n">idx</span><span class="p">))</span>

        <span class="c1"># Add feat_size to the head of classifier_layer_dims (the output of</span>
        <span class="c1"># the PointNet feature extractor has number of elements equal to</span>
        <span class="c1"># has number of channels equal to `in_channels`).</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">classifier_layer_dims</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="n">classifier_layer_dims</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">classifier_layer_dims</span><span class="p">)</span>
        <span class="n">classifier_layer_dims</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">feat_size</span><span class="p">)</span>

        <span class="c1"># Note that `global_feat` MUST be set to False, for</span>
        <span class="c1"># segmentation tasks.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">feature_extractor</span> <span class="o">=</span> <span class="n">PointNetFeatureExtractor</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">feat_size</span><span class="o">=</span><span class="n">feat_size</span><span class="p">,</span>
            <span class="n">layer_dims</span><span class="o">=</span><span class="n">feat_layer_dims</span><span class="p">,</span> <span class="n">global_feat</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span> <span class="n">batchnorm</span><span class="o">=</span><span class="n">batchnorm</span><span class="p">,</span>
            <span class="n">transposed_input</span><span class="o">=</span><span class="n">transposed_input</span>
        <span class="p">)</span>

        <span class="c1"># Compute the dimensionality of local features</span>
        <span class="c1"># Local feature size = (global feature size) + (feature size</span>
        <span class="c1">#       from the output of the first layer of feature extractor)</span>
        <span class="c1"># Note: In self.feature_extractor, we manually append in_channels</span>
        <span class="c1"># to the head of feat_layer_dims. So, we use index 1 of</span>
        <span class="c1"># feat_layer_dims in the below line, to compute local_feat_size,</span>
        <span class="c1"># and not index 0.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">local_feat_size</span> <span class="o">=</span> <span class="n">feat_size</span> <span class="o">+</span> <span class="n">feat_layer_dims</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">conv_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">batchnorm</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bn_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
        <span class="c1"># First classifier layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">local_feat_size</span><span class="p">,</span>
                                          <span class="n">classifier_layer_dims</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">batchnorm</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bn_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">classifier_layer_dims</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">classifier_layer_dims</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">conv_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="n">classifier_layer_dims</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span>
                                              <span class="n">classifier_layer_dims</span><span class="p">[</span><span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">batchnorm</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">bn_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span>
                    <span class="n">classifier_layer_dims</span><span class="p">[</span><span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">last_conv_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="n">classifier_layer_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                                         <span class="n">num_classes</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Store activation as a class attribute</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">activation</span>

        <span class="c1"># Store the number of classes as an attribute</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">=</span> <span class="n">num_classes</span>

        <span class="c1"># Store whether or not to use batchnorm as a class attribute</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batchnorm</span> <span class="o">=</span> <span class="n">batchnorm</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">transposed_input</span> <span class="o">=</span> <span class="n">transposed_input</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Forward pass through the PointNet segmentation model.</span>

<span class="sd">        Args:</span>
<span class="sd">            x (torch.Tensor): Tensor representing a pointcloud</span>
<span class="sd">                shape: :math:`B \times N \times D`, where :math:`B`</span>
<span class="sd">                is the batchsize, :math:`N` is the number of points</span>
<span class="sd">                in the pointcloud, and :math:`D` is the dimensionality</span>
<span class="sd">                of each point in the pointcloud.</span>
<span class="sd">                If self.transposed_input is True, then the shape is</span>
<span class="sd">                :math:`B \times D \times N`.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">batchsize</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">num_points</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">transposed_input</span> <span class="k">else</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_extractor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv_layers</span><span class="p">)):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batchnorm</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bn_layers</span><span class="p">[</span><span class="n">idx</span><span class="p">](</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">conv_layers</span><span class="p">[</span><span class="n">idx</span><span class="p">](</span><span class="n">x</span><span class="p">)))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv_layers</span><span class="p">[</span><span class="n">idx</span><span class="p">](</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_conv_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        <span class="c1"># x = F.log_softmax(x.view(-1, self.num_classes), dim=-1)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;x.shape = </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batchsize</span><span class="p">,</span> <span class="n">num_points</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">)</span></div>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2019, NVIDIA Development Inc.

    </p>
  </div>
    
    
      Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>